from os import listdir, makedirs
from os.path import join, splitext, exists

from gensim.models import Word2Vec

from bugfinder.base.processing import AbstractProcessing
from bugfinder.settings import LOGGER


class Word2VecModel(AbstractProcessing):
    """Class responsible for the training of the Word2Vec model using the corpus
    generated by the tokenization of the dataset.
    """

    tokens = {}

    word_dim = 50
    window_dim = 5
    min_count = 1
    workers = 4
    algorithm = 1  # 1 = skipgram
    seed = 32

    def execute(self, name, **kwargs):
        """Run the processing. This function reads each tokenized file in the dataset,
        generates a corpus, trains the model and saves it.

        Args:
            name (str): This parameter will be the name of the model saved in disk.
        """
        LOGGER.debug("Generating the token list for training...")

        token_list = self.get_token_list()

        LOGGER.debug("Training the word2vec model.")

        model = Word2Vec(
            token_list,
            min_count=self.min_count,
            vector_size=self.word_dim,
            workers=self.workers,
            sg=self.algorithm,
            seed=self.seed,
        )

        LOGGER.debug("Training complete. Saving the model...")

        model_dir = join(self.dataset.model_dir, name)

        if not exists(self.dataset.model_dir):
            makedirs(self.dataset.model_dir)

        model.save(model_dir)

    def get_token_list(self):
        """Reads each file, retrieves the tokens from it and concatenates them in a
        single list which will be the corpus.

        Returns:
            token_list: list containing all the tokens in the dataset, concatenated
            to form the corpus
        """
        token_list = list()

        file_processing_list = [
            join(test_case, filepath)
            for test_case in self.dataset.test_cases
            for filepath in listdir(join(self.dataset.path, test_case))
            if splitext(filepath)[1] in [".c", ".h"]
        ]

        while len(file_processing_list) != 0:
            filepath = file_processing_list.pop(0)

            with open(join(self.dataset.path, filepath), "r") as in_file:
                code = in_file.readlines()

                tokens = [token.strip() for token in code]
                token_list.append(tokens)

        return token_list
