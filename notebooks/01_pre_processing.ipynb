{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Pre-processing\n",
    "\n",
    "This notebook will pre-process a classified C/C++ dataset specifically designed for bugfinding classification to ensure correct formatting before the Joern parsing.\n",
    "\n",
    "Download the dataset using the script at `../scripts/setup_ai_dataset.sh`. A new folder **data/ai-dataset_orig** should appear, containing the classified dataset with *bad* (buggy) and *good* (fixed) classes.\n",
    "\n",
    "## 01.a. Imports and logging configuration\n",
    "\n",
    "The first step is to perform the necessary imports and configure the program. Additionally, if the dataset need to be downloaded, it can be done in the last cell of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable these line if live changes in the codebase are made\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific instruction to run the notebooks from a sub-folder.\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from os.path import join\n",
    "from bugfinder.settings import LOGGER\n",
    "from bugfinder.base.dataset import CodeWeaknessClassificationDataset as Dataset\n",
    "from bugfinder.processing.dataset.copy import CopyDataset\n",
    "from bugfinder.processing.dataset.extract import ExtractSampleDataset\n",
    "from bugfinder.processing.cleaning.remove_main_function import RemoveMainFunction\n",
    "from bugfinder.processing.cleaning.replace_litterals import ReplaceLitterals\n",
    "from bugfinder.processing.cleaning.remove_cpp_files import RemoveCppFiles\n",
    "from bugfinder.processing.cleaning.remove_interproc_files import RemoveInterprocFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging to only output INFO level messages\n",
    "LOGGER.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directories (DO NOT EDIT)\n",
    "classified_dataset_path = \"../data/ai-dataset_orig\"\n",
    "cleaned_dataset_path = \"../data/ai-dataset_cleaned\"\n",
    "subset_dataset_path = \"../data/ai-dataset_v000\"\n",
    "\n",
    "# Number of sample to test (edit this number, performances will be impacted)\n",
    "sample_nb = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Step: Download the dataset\n",
    "\n",
    "Use the following cell to download the dataset. The cell needs to be run only if the dataset is not present or has been tampered with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset and classify the samples between good and bad classes.\n",
    "import subprocess\n",
    "from os import listdir\n",
    "from os.path import isdir\n",
    "\n",
    "force_download = False  # Change to True if the dataset has been tampered with\n",
    "download_dir = join(classified_dataset_path, \"bad\")\n",
    "need_download = (not isdir(download_dir) or len(listdir(download_dir)) != 6507)\n",
    "\n",
    "if need_download or force_download:\n",
    "    LOGGER.info(\"Downloading dataset...\")\n",
    "    subprocess.run(\"../scripts/setup_ai_dataset.sh\")\n",
    "\n",
    "LOGGER.info(\"Dataset has been downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.b. Cleanup\n",
    "\n",
    "Cleanup the downloaded data to ensure correct parsing in the future steps. The dataset will be stored in **./data/ai-dataset_cleaned**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the annotated dataset to avoid overwriting\n",
    "classified_dataset = Dataset(classified_dataset_path)\n",
    "classified_dataset.queue_operation(CopyDataset, {\"to_path\": cleaned_dataset_path, \"force\": True})\n",
    "classified_dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup new dataset\n",
    "cleaned_dataset = Dataset(cleaned_dataset_path)\n",
    "\n",
    "cleaned_dataset.queue_operation(RemoveCppFiles)\n",
    "cleaned_dataset.queue_operation(RemoveInterprocFiles)\n",
    "cleaned_dataset.queue_operation(RemoveMainFunction)\n",
    "cleaned_dataset.queue_operation(ReplaceLitterals)\n",
    "\n",
    "cleaned_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.c. Subset extraction\n",
    "\n",
    "Extract a subset of the data for testing purposes at **./data/ai-dataset_v000**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a subset of 1000 samples for training, test and validation purposes. \n",
    "cleaned_dataset = Dataset(cleaned_dataset_path)\n",
    "cleaned_dataset.queue_operation(\n",
    "    ExtractSampleDataset, {\"to_path\": subset_dataset_path, \"sample_nb\": sample_nb, \"force\": True}\n",
    ")\n",
    "cleaned_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this part, the initial dataset was cleaned and is now ready to be processed by Joern. The [next notebook](./02_joern_processing.ipynb) details the step to run Joern and import the dataset into a Neo4J database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
