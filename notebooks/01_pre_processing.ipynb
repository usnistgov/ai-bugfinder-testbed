{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Pre-processing\n",
    "\n",
    "This notebook will pre-process a classified C/C++ dataset extracted from [Juliet 1.3](https://samate.nist.gov/SRD/testsuite.php) to ensure correct formatting before the Joern parsing.\n",
    "\n",
    "Download the dataset using the script at `../scripts/download_cwe121.sh`. In the **data** directory, the following directories should be present:\n",
    "* **cwe121_annot**: classified dataset with bad (buggy) and good (fixed) classes.\n",
    "* **cwe121_orig**: original dataset, unzipped version of the next file.\n",
    "* **Juliet_Test_Suite_v1.3_for_C_Cpp.zip**: dataset downloaded from the SARD website.\n",
    "\n",
    "Use the following cell to download the dataset. Beware that <u>it will use all available CPUs</u>. The cell needs to be run only if the dataset is not present or has been tampered with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWE-121 downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download the CWE-121 from Juliet 1.3 and classify the samples between good and bad classes.\n",
    "# /!\\ RUN ONLY ONCE /!\\\n",
    "import subprocess\n",
    "from os import listdir\n",
    "from os.path import isdir\n",
    "\n",
    "force_download = False\n",
    "download_dir = \"../data/cwe121_annot/bad\"\n",
    "need_download = (not isdir(download_dir) or len(listdir(download_dir)) != 4944)\n",
    "\n",
    "if need_download or force_download:\n",
    "    print(\"Downloading CWE-121...\")\n",
    "    subprocess.run(\"../scripts/download_cwe121.sh\")\n",
    "\n",
    "print(\"CWE-121 downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.a. Imports and logging configuration\n",
    "\n",
    "Once the dataset is downloaded, the next step is to perform the necessary imports and configure the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific instruction to run the notebooks from a sub-folder.\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tools.settings import LOGGER\n",
    "from tools.dataset import CWEClassificationDataset as Dataset\n",
    "from tools.dataset.processing.content_ops import RemoveMainFunction, ReplaceLitterals\n",
    "from tools.dataset.processing.dataset_ops import CopyDataset, ExtractSampleDataset\n",
    "from tools.dataset.processing.file_ops import RemoveCppFiles, RemoveInterproceduralTestCases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging to only output INFO level messages\n",
    "LOGGER.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directories (DO NOT EDIT)\n",
    "classified_dataset_path = \"../data/cwe121_annot\"\n",
    "cleaned_dataset_path = \"../data/cwe121_dataset\"\n",
    "cwe121_dataset_path = \"../data/cwe121_training_orig\"\n",
    "\n",
    "# Number of sample to test (edit this number, performances will be impacted, max. 6288)\n",
    "sample_nb = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.b. Cleanup\n",
    "\n",
    "Cleanup the downloaded data to ensure correct parsing in the future steps. The dataset will be stored in **./data/cwe121_dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-02 18:07:44][INFO] Dataset index build in 429ms. 9888 test_cases, 2 classes, 0 features (v0).\n",
      "[2019-12-02 18:07:44][INFO] Running operation 1/1 (CopyDataset)...\n",
      "[2019-12-02 18:07:44][INFO] Dataset index build in 54ms. 1666 test_cases, 2 classes, 0 features (v0).\n",
      "[2019-12-02 18:07:44][INFO] Running operation 1/1 (RightFixer)...\n",
      "[2019-12-02 18:07:45][INFO] 1 operations run in 1064ms.\n",
      "[2019-12-02 18:07:48][INFO] 1 operations run in 4345ms.\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the annotated dataset to avoid overwriting\n",
    "classified_dataset = Dataset(classified_dataset_path)\n",
    "classified_dataset.queue_operation(CopyDataset, {\"to_path\": cleaned_dataset_path, \"force\": True})\n",
    "classified_dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-02 18:07:53][INFO] Dataset index build in 413ms. 9888 test_cases, 2 classes, 0 features (v0).\n",
      "[2019-12-02 18:07:53][INFO] Running operation 1/4 (RemoveCppFiles)...\n",
      "[2019-12-02 18:07:54][INFO] Dataset index build in 316ms. 8684 test_cases, 2 classes, 0 features (v0).\n",
      "[2019-12-02 18:07:54][INFO] Running operation 2/4 (RemoveInterproceduralTestCases)...\n",
      "[2019-12-02 18:07:55][INFO] Dataset index build in 231ms. 6288 test_cases, 2 classes, 0 features (v0).\n",
      "[2019-12-02 18:07:55][INFO] Running operation 3/4 (RemoveMainFunction)...\n",
      "[2019-12-02 18:07:57][INFO] Dataset index build in 208ms. 6288 test_cases, 2 classes, 0 features (v0).\n",
      "[2019-12-02 18:07:57][INFO] Running operation 4/4 (ReplaceLitterals)...\n",
      "[2019-12-02 18:08:04][INFO] 4 operations run in 11119ms.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup new dataset\n",
    "cleaned_dataset = Dataset(cleaned_dataset_path)\n",
    "\n",
    "cleaned_dataset.queue_operation(RemoveCppFiles)\n",
    "cleaned_dataset.queue_operation(RemoveInterproceduralTestCases)\n",
    "cleaned_dataset.queue_operation(RemoveMainFunction)\n",
    "cleaned_dataset.queue_operation(ReplaceLitterals)\n",
    "\n",
    "cleaned_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.c. Subset extraction\n",
    "\n",
    "Extract a subset of the data for testing purposes at **./data/cwe121_training_orig**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-02 18:08:10][INFO] Running operation 1/1 (ExtractSampleDataset)...\n",
      "[2019-12-02 18:08:10][INFO] 1 operations run in 119ms.\n",
      "[2019-12-02 18:08:10][INFO] Dataset index build in 8ms. 200 test_cases, 2 classes, 0 features (v0).\n"
     ]
    }
   ],
   "source": [
    "# Extract a subset of 1000 samples for training, test and validation purposes. \n",
    "cleaned_dataset.queue_operation(\n",
    "    ExtractSampleDataset, {\"to_path\": cwe121_dataset_path, \"sample_nb\": sample_nb, \"force\": True}\n",
    ")\n",
    "cleaned_dataset.process()\n",
    "\n",
    "# Open the dataset to see its statistics.\n",
    "cwe121_dataset = Dataset(cwe121_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this part, the initial dataset wascleaned and is now ready to be processed by Joern. The [next notebook](./02_joern_processing.ipynb) details the step to run Joern and import the dataset into a Neo4J database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
