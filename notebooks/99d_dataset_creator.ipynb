{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Dataset Creator (experimental)\n",
    "\n",
    "A notebook to create datasets containing a subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable these line if live changes in the codebase are made\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tensorflow logging\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # any {'0', '1', '2', '3'}\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific instruction to run the notebooks from a sub-folder.\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bugfinder.settings import LOGGER\n",
    "from bugfinder.dataset import CWEClassificationDataset as Dataset\n",
    "from bugfinder.models.dnn_classifier import DNNClassifierTraining\n",
    "from bugfinder.models.linear_classifier import LinearClassifierTraining\n",
    "from bugfinder.dataset.processing.dataset_ops import CopyDataset, RightFixer\n",
    "from bugfinder.features.any_hop.all_flows import FeatureExtractor as AnyHopAllFlowsExtractor\n",
    "from bugfinder.features.any_hop.single_flow import FeatureExtractor as AnyHopSingleFlowExtractor\n",
    "from bugfinder.features.single_hop.raw import FeatureExtractor as SingleHopRawExtractor\n",
    "from bugfinder.features.pca import FeatureExtractor as PCA\n",
    "\n",
    "from os.path import join, exists, basename, dirname\n",
    "from shutil import rmtree, copytree\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bugfinder.dataset.processing import DatasetProcessing, DatasetProcessingCategory\n",
    "from bugfinder.settings import LOGGER\n",
    "from bugfinder.utils.statistics import has_better_metrics\n",
    "from os.path import join, exists\n",
    "from shutil import rmtree, copytree\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "from xlsxwriter.utility import xl_rowcol_to_cell\n",
    "import sklearn.decomposition\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging to only output INFO level messages\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "LOGGER.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/mnt/data/ai-bugfinder/aiwe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Global parameters\n",
    "output_path = \"/mnt/data/aiwe\"\n",
    "# dataset_path = \"../data/ds-rev-pca\"\n",
    "# tensorboard_path = \"/home/pnd/tb-test\"\n",
    "# dataset = Dataset(dataset_path)\n",
    "\n",
    "# model_config = [10, 10, 10]\n",
    "# batch_size=250\n",
    "# training_epochs=5\n",
    "\n",
    "# orig_cols = list(dataset.features.columns)\n",
    "\n",
    "# # Remove result columns\n",
    "# df = dataset.features\n",
    "# df = df.drop(orig_cols[-2:], axis=1)\n",
    "\n",
    "# orig_cols = orig_cols[:-2]\n",
    "feature_file = \"/mnt/data/ai-bugfinder/aiwe-all.csv\"\n",
    "df = pd.read_csv(feature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_path}/output.csv\", \"w\") as csv_file:\n",
    "    df[\"result\"].to_csv(csv_file, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "feat_df = df.drop(cols[-2:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_path}/default.csv\", \"w\") as csv_file:\n",
    "    feat_df.to_csv(csv_file, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = df.std().to_numpy()\n",
    "for std_limit in [1e-2, 1e-1]:\n",
    "    std_cols = [df.columns[std_idx] for std_idx in range(len(std)) if std[std_idx] >= std_limit]\n",
    "\n",
    "    df_significant = df[std_cols]\n",
    "\n",
    "    with open(f\"{output_path}/sign{str(std_limit).replace('.', '')}.csv\", \"w\") as csv_file:\n",
    "        df_significant.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bags(bag_count, seed=None):\n",
    "    item_count = round(len(orig_cols) / bag_count) + 1\n",
    "    print(f\"Creating {bag_count} bags containing {item_count} features...\")\n",
    "    \n",
    "    copy_cols = deepcopy(orig_cols)\n",
    "    \n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(copy_cols)\n",
    "        \n",
    "    column_bags = [\n",
    "        copy_cols[bag_index*item_count:(bag_index+1)*item_count] for bag_index in range(bag_count)\n",
    "    ]\n",
    "    del copy_cols\n",
    "\n",
    "    print(f\"Creating list of features...\")\n",
    "    df_list = [df[columns] for columns in column_bags]\n",
    "    \n",
    "    for df_idx in range(len(df_list)):\n",
    "        filename = f\"bag{df_idx:02d}of{bag_count}with{item_count}features{seed}random\"\n",
    "        with open(f\"{output_path}/{filename}.csv\", \"w\") as csv_file:\n",
    "            df_list[df_idx].to_csv(csv_file, index=None)\n",
    "            \n",
    "        std = df_list[df_idx].std().to_numpy()\n",
    "        for std_limit in [1e-2, 1e-1]:\n",
    "            std_cols = [\n",
    "                df_list[df_idx].columns[std_idx] \n",
    "                for std_idx in range(len(std)) if std[std_idx] >= std_limit\n",
    "            ]\n",
    "            \n",
    "            if len(std_cols) < 50:\n",
    "                continue\n",
    "\n",
    "            df_significant = df_list[df_idx][std_cols]\n",
    "\n",
    "            with open(f\"{output_path}/{filename}{std_limit}.csv\", \"w\") as csv_file:\n",
    "                df_significant.to_csv(csv_file, index=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(101)\n",
    "bags = [10, 20, 30, 40, 50]\n",
    "seeds = [\n",
    "    random.randint(10, 99) for x in range(4) \n",
    "]\n",
    "print(f\"Seeds: {str(seeds)}\")\n",
    "\n",
    "for bag_count in bags:\n",
    "    get_bags(bag_count)\n",
    "    \n",
    "    for seed in seeds:\n",
    "        get_bags(bag_count, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = 50\n",
    "\n",
    "def get_pca_weights(X, n_comp):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    pca = sklearn.decomposition.PCA(n_components=n_comp)\n",
    "    pca.fit(X)\n",
    "    \n",
    "    return pca, mu\n",
    "    \n",
    "print(\"Calculating PCA operations...\")\n",
    "pca_compute_list = [\n",
    "    get_pca_weights(df, pca_components) for df in df_list   \n",
    "]\n",
    "\n",
    "print(\"Computing PCA...\")\n",
    "df_list_pca = [\n",
    "    pca_compute_list[idx][0].transform(df_list[idx])  \n",
    "    for idx in range(len(df_list))\n",
    "]\n",
    "\n",
    "# print(\"Reversing PCA...\")\n",
    "# df_list_pca_rev = [\n",
    "#     np.dot(\n",
    "#         df_list_pca[idx][:,:pca_components], \n",
    "#         pca_compute_list[idx][0].components_[:pca_components,:]\n",
    "#     ) + pca_compute_list[idx][1]\n",
    "#     for idx in range(len(df_list))\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_data, output_data, model_config, model_dir, batch_size, training_epochs, max_training_items=None):\n",
    "    # Renaming input columns to avoid forbidden characters\n",
    "    columns = [\n",
    "        f\"feat{feature_nb:03d}\" for feature_nb in range(input_data.shape[1])\n",
    "    ]\n",
    "    input_data = pd.DataFrame(input_data, columns=columns)\n",
    "\n",
    "    # Splitting into training set and test set\n",
    "    input_train, input_test, output_train, output_test = train_test_split(\n",
    "        input_data, output_data, test_size=0.33, random_state=101\n",
    "    )\n",
    "\n",
    "    train_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        x=input_train,\n",
    "        y=output_train,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    test_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        x=input_test, y=output_test, shuffle=False, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Creating the model\n",
    "    model = tf.estimator.DNNClassifier(\n",
    "        hidden_units = model_config,\n",
    "        feature_columns=[\n",
    "            tf.feature_column.numeric_column(col) for col in columns\n",
    "        ],\n",
    "        n_classes=2,\n",
    "        model_dir=model_dir,\n",
    "    )\n",
    "\n",
    "    LOGGER.debug(\n",
    "        \"Training %s on %d samples and testing on %d samples...\" % (\n",
    "            model.__class__.__name__, input_train.shape[0], input_test.shape[0]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Train the model and evaluate for the given number of epochs\n",
    "    for epoch_num in range(training_epochs):  \n",
    "        LOGGER.debug(\"Training dataset for epoch %d/%d...\" % (epoch_num + 1, training_epochs))\n",
    "        model.train(input_fn=train_fn, steps=max_training_items)\n",
    "        preds = model.evaluate(input_fn=test_fn)\n",
    "        \n",
    "    return 2 * preds[\"precision\"] * preds[\"recall\"] / (preds[\"precision\"] + preds[\"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LOGGER.info(\"Training %s...\" % dataset_path)\n",
    "scores = list()\n",
    "\n",
    "for df_idx in range(len(df_list_pca)):\n",
    "    print(f\"Training with dataset {df_idx+1}/{len(df_list_pca)}\")\n",
    "    \n",
    "    score = train(\n",
    "        df_list_pca[df_idx],\n",
    "        dataset.features[\"result\"],\n",
    "        model_config,\n",
    "        None, \n",
    "        batch_size, \n",
    "        training_epochs * 3\n",
    "    )\n",
    "    scores.append(score)\n",
    "\n",
    "print(scores)\n",
    "print(\"Job done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(dataset_path, input_data, model_config, model_dir):\n",
    "    LOGGER.info(f\"Loading dataset at {dataset_path}\")\n",
    "    dataset = Dataset(dataset_path)\n",
    "    \n",
    "    columns = [\n",
    "        f\"feat{feature_nb:03d}\" for feature_nb in range(input_data.shape[1])\n",
    "    ]\n",
    "    LOGGER.info(f\"Found {len(columns)} columns\")\n",
    "\n",
    "    model = tf.estimator.DNNClassifier(\n",
    "        hidden_units = model_config,\n",
    "        feature_columns=[\n",
    "            tf.feature_column.numeric_column(col) for col in columns\n",
    "        ],\n",
    "        n_classes=2,\n",
    "        model_dir=model_dir,\n",
    "    )\n",
    "\n",
    "    return model, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload trained model\n",
    "chosen_model, model_cols = init_model(\n",
    "    dataset_path,\n",
    "    pcaX,\n",
    "    model_config,\n",
    "    join(tensorboard_path, basename(dataset_path))\n",
    ")\n",
    "LOGGER.info(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model.get_variable_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model_vars = [var for var in chosen_model.get_variable_names() if basename(var) == \"kernel\"]\n",
    "layers = sorted(set([basename(dirname(var)) for var in chosen_model_vars]))\n",
    "\n",
    "weights = dict()\n",
    "cols = model_cols\n",
    "\n",
    "for layer in layers:    \n",
    "    kernel = chosen_model.get_variable_value(f\"dnn/{layer}/kernel\").transpose()\n",
    "    \n",
    "    layer_weights = np.zeros((kernel.shape[0], kernel.shape[1]+1))\n",
    "    \n",
    "    layer_weights[:, :-1] = kernel\n",
    "    layer_weights[:, -1] = chosen_model.get_variable_value(f\"dnn/{layer}/bias\")\n",
    "    \n",
    "    layer_name = f\"dnn/{layer}/bias\"\n",
    "    print(f\"Layer size for {layer_name}: {chosen_model.get_variable_value(layer_name).shape}\")\n",
    "    \n",
    "    weights[layer] = pd.DataFrame(layer_weights, columns=cols+[\"bias\"])\n",
    "    \n",
    "    cols = [f\"{layer}_n{index}\" for index in range(layer_weights.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "randX = df.sample(n=25, random_state=101)\n",
    "LOGGER.info(f\"Testing with matrix {randX.shape}...\")\n",
    "\n",
    "pca_randX = pca.transform(randX)\n",
    "LOGGER.info(f\"Matrix size after PCA: {pca_randX.shape}\")\n",
    "\n",
    "pca_randX = pd.DataFrame(pca_randX, columns=model_cols)\n",
    "\n",
    "input_fn = tf.estimator.inputs.pandas_input_fn(x=pca_randX, shuffle=False)\n",
    "predictions = list(chosen_model.predict(input_fn))\n",
    "\n",
    "randX = randX.to_numpy()\n",
    "pca_randX = pca_randX.to_numpy()\n",
    "print(predictions[0])\n",
    "LOGGER.info(f\"Predictions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = \"/mnt/data/ai_bugfinder/model-weights.xlsx\"\n",
    "\n",
    "LOGGER.info(f\"Starting writing {basename(workbook)}...\")\n",
    "workbook  = xlsxwriter.Workbook(workbook)\n",
    "\n",
    "start_row = 3\n",
    "start_col = 1\n",
    "\n",
    "LOGGER.info(\"Writing sample data...\")\n",
    "worksheet = workbook.add_worksheet(\"Sample data\")\n",
    "\n",
    "row_idx = start_row\n",
    "\n",
    "for row in randX:\n",
    "    worksheet.write(row_idx, 0, f\"item{row_idx-start_row}\")\n",
    "    \n",
    "    col_idx = start_col\n",
    "    \n",
    "    for item in row:\n",
    "        worksheet.write(row_idx, col_idx, item)\n",
    "        col_idx += 1\n",
    "        \n",
    "    row_idx += 1\n",
    "    \n",
    "row_idx += start_row\n",
    "\n",
    "for row in pca_randX:\n",
    "    worksheet.write(row_idx, 0, f\"item{row_idx-2*start_row-len(randX)}\")\n",
    "    \n",
    "    col_idx = start_col\n",
    "    \n",
    "    for item in row:\n",
    "        worksheet.write(row_idx, col_idx, item)\n",
    "        col_idx += 1\n",
    "        \n",
    "    row_idx += 1\n",
    "    \n",
    "row_idx += start_row\n",
    "\n",
    "for row in predictions:\n",
    "    worksheet.write(row_idx, 0, f\"item{row_idx-3*start_row-len(randX)*2}\")\n",
    "    \n",
    "    col_idx = start_col\n",
    "    \n",
    "    for item in row[\"probabilities\"]:\n",
    "        worksheet.write(row_idx, col_idx, item)\n",
    "        col_idx += 1   \n",
    "    \n",
    "    worksheet.write(row_idx, col_idx, row[\"logits\"])\n",
    "    col_idx += 1\n",
    "\n",
    "    worksheet.write(row_idx, col_idx, row[\"logistic\"])\n",
    "    col_idx += 1\n",
    "        \n",
    "    row_idx += 1\n",
    "\n",
    "\n",
    "LOGGER.info(f\"Writing PCA weights...\")\n",
    "worksheet = workbook.add_worksheet(\"PCA weights\")\n",
    "cols = dataset.features.columns[:-2]\n",
    "\n",
    "for x in range(pca_weights.shape[0]-1):\n",
    "    worksheet.write(x+3, 0, f\"pca{x}\")\n",
    "    \n",
    "worksheet.write(pca_weights.shape[0]+2, 0, \"mu\")\n",
    "\n",
    "for y in range(len(cols)):\n",
    "    worksheet.write(2, y+1, cols[y])\n",
    "\n",
    "row_idx = start_row\n",
    "\n",
    "for row in pca_weights:\n",
    "    col_idx = 1\n",
    "    row_avg = np.mean(row)\n",
    "    \n",
    "    for item in row:\n",
    "        worksheet.write(row_idx, col_idx, item)\n",
    "        col_idx += 1\n",
    "        \n",
    "    start = xl_rowcol_to_cell(row_idx, 1)\n",
    "    end = xl_rowcol_to_cell(row_idx, col_idx-1)    \n",
    "    \n",
    "    worksheet.conditional_format(f\"{start}:{end}\", {\n",
    "        \"type\": \"3_color_scale\",\n",
    "        \"min_color\": \"blue\",\n",
    "        \"mid_color\": \"white\",\n",
    "        \"max_color\": \"red\"\n",
    "    })\n",
    "    row_idx += 1\n",
    "\n",
    "LOGGER.info(f\"Writing DNN weights...\")\n",
    "worksheet = workbook.add_worksheet(\"DNN weights\")\n",
    "\n",
    "row_idx = start_row\n",
    "\n",
    "for layer, weight_df in weights.items():\n",
    "    col_idx = start_col\n",
    "        \n",
    "    if row_idx != start_row:\n",
    "        row_idx -= len(weight_df.columns) + start_row - 1\n",
    "        \n",
    "        for col in weight_df.columns[:-1]:\n",
    "            row_idx += 1\n",
    "            worksheet.write(row_idx, 0, col)\n",
    "            \n",
    "        row_idx += start_row\n",
    "    \n",
    "    for col in weight_df.columns:\n",
    "        worksheet.write(row_idx, col_idx, col)\n",
    "        col_idx += 1\n",
    "        \n",
    "        \n",
    "    for index, item in weight_df.iterrows():\n",
    "        row_idx += 1\n",
    "        col_idx = start_col\n",
    "        \n",
    "        for item_val in item.values:\n",
    "            worksheet.write(row_idx, col_idx, item_val)\n",
    "            col_idx += 1\n",
    "            \n",
    "        start = xl_rowcol_to_cell(row_idx, 1)\n",
    "        end = xl_rowcol_to_cell(row_idx, col_idx-1)\n",
    "        worksheet.conditional_format(f\"{start}:{end}\", {\n",
    "            \"type\": \"3_color_scale\",\n",
    "            \"min_color\": \"blue\",\n",
    "            \"mid_color\": \"white\",\n",
    "            \"max_color\": \"red\"\n",
    "        })\n",
    "        \n",
    "    row_idx += start_row\n",
    "    \n",
    "\n",
    "workbook.close()\n",
    "LOGGER.info(\"Job done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
