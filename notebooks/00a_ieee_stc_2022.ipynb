{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE STC 2022 - A Modular and Expandable Testbed for Evaluating ML-Based Bug Finders - Software demo\n",
    "\n",
    "This notebook will pre-process a C/C++ dataset specifically designed for bugfinding classification to ensure correct formatting before the Joern parsing.\n",
    "\n",
    "Download the dataset using the script at `../scripts/setup_ai_dataset.sh`. A new folder **data/ai-dataset_orig** should appear, containing the classified dataset with *bad* (buggy) and *good* (fixed) classes.\n",
    "\n",
    "## 01 Pre-processing\n",
    "\n",
    "### 01.a. Imports and logging configuration\n",
    "\n",
    "The first step is to perform the necessary imports and configure the program. Additionally, if the dataset need to be downloaded, it can be done in the last cell of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable these line if live changes in the codebase are made\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tensorflow logging\n",
    "import os\n",
    "import logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific instruction to run the notebooks from a sub-folder.\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from os.path import join\n",
    "from bugfinder.settings import LOGGER\n",
    "from bugfinder.base.dataset import CodeWeaknessClassificationDataset as Dataset\n",
    "from bugfinder.processing.dataset.copy import CopyDataset\n",
    "from bugfinder.processing.dataset.fix_rights import RightFixer\n",
    "from bugfinder.processing.dataset.extract import ExtractSampleDataset\n",
    "from bugfinder.processing.cleaning.remove_main_function import RemoveMainFunction\n",
    "from bugfinder.processing.cleaning.replace_litterals import ReplaceLitterals\n",
    "from bugfinder.processing.cleaning.remove_cpp_files import RemoveCppFiles\n",
    "from bugfinder.processing.cleaning.remove_interproc_files import RemoveInterprocFiles\n",
    "from bugfinder.processing.joern.v040 import JoernProcessing as Joern040DatasetProcessing\n",
    "from bugfinder.processing.neo4j.importer import Neo4J3Importer\n",
    "from bugfinder.processing.neo4j.annot import Neo4JAnnotations\n",
    "from bugfinder.processing.ast.v02 import Neo4JASTMarkup as Neo4JASTMarkupV02\n",
    "from bugfinder.features.extraction.bag_of_words.hops_n_flows import FeatureExtractor as HopsNFlowsExtractor\n",
    "from bugfinder.features.reduction.pca import FeatureSelector as PCA\n",
    "from bugfinder.models.dnn_classifier import DNNClassifierTraining\n",
    "from bugfinder.models.linear_classifier import LinearClassifierTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging to only output INFO level messages\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "LOGGER.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directories (DO NOT EDIT)\n",
    "classified_dataset_path = \"../data/ai-dataset_orig\"\n",
    "cleaned_dataset_path = \"../data/dataset_01_clean\"\n",
    "sample_dataset_path = \"../data/dataset_01_extract\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Step: Download the dataset\n",
    "\n",
    "Use the following cell to download the dataset. The cell needs to be run only if the dataset is not present or has been tampered with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset and classify the samples between good and bad classes.\n",
    "import subprocess\n",
    "from os import listdir\n",
    "from os.path import isdir\n",
    "\n",
    "force_download = False  # Change to True if the dataset has been tampered with\n",
    "download_dir = join(classified_dataset_path, \"bad\")\n",
    "need_download = (not isdir(download_dir) or len(listdir(download_dir)) != 6507)\n",
    "\n",
    "if need_download or force_download:\n",
    "    LOGGER.info(\"Downloading dataset...\")\n",
    "    subprocess.run(\"../scripts/setup_ai_dataset.sh\")\n",
    "\n",
    "LOGGER.info(\"Dataset has been downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.b. Cleanup\n",
    "\n",
    "Cleanup the downloaded data to ensure correct parsing in the future steps. The dataset will be stored in **./data/ai-dataset_cleaned**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the annotated dataset to avoid overwriting\n",
    "classified_dataset = Dataset(classified_dataset_path)\n",
    "classified_dataset.queue_operation(CopyDataset, {\"to_path\": cleaned_dataset_path, \"force\": True})\n",
    "classified_dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup new dataset\n",
    "cleaned_dataset = Dataset(cleaned_dataset_path)\n",
    "\n",
    "cleaned_dataset.queue_operation(RemoveCppFiles)\n",
    "cleaned_dataset.queue_operation(RemoveInterprocFiles)\n",
    "cleaned_dataset.queue_operation(RemoveMainFunction)\n",
    "cleaned_dataset.queue_operation(ReplaceLitterals)\n",
    "\n",
    "cleaned_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.c. Subset extraction\n",
    "\n",
    "Extract a subset of the data for testing purposes at **./data/ai-dataset_v000**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sample to test (edit this number, performances will be impacted)\n",
    "sample_nb = 50\n",
    "\n",
    "cleaned_dataset = Dataset(cleaned_dataset_path)\n",
    "cleaned_dataset.queue_operation(\n",
    "    ExtractSampleDataset, {\"to_path\": sample_dataset_path, \"sample_nb\": sample_nb, \"force\": True}\n",
    ")\n",
    "cleaned_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Pre-processing\n",
    "\n",
    "In this part, the previously created dataset will be parsed using various version of Joern. The parsed data will then be imported or converted into a Neo4J v3 database for further processing. Once the data is in a Neo4J database, an AST representation is extracted to be used by feature extraction algorithms.\n",
    "\n",
    "### 02.a. Dataset preparation\n",
    "\n",
    "A copy of the dataset is created before peforming any of the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joern_dataset_path = \"../data/dataset_02_joern\"\n",
    "\n",
    "sample_dataset = Dataset(sample_dataset_path)\n",
    "sample_dataset.queue_operation(CopyDataset, {\"to_path\": joern_dataset_path, \"force\": True})\n",
    "\n",
    "sample_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.b. Joern v0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset that is going to be used\n",
    "joern_dataset = Dataset(joern_dataset_path)\n",
    "\n",
    "# Apply Joern 4.0 conversion and import into Neo4J v3\n",
    "joern_dataset.queue_operation(Joern040DatasetProcessing)\n",
    "joern_dataset.queue_operation(Neo4J3Importer)\n",
    "joern_dataset.queue_operation(Neo4JAnnotations)\n",
    "joern_dataset.queue_operation(Neo4JASTMarkupV02)\n",
    "\n",
    "joern_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Feature extraction and reduction\n",
    "\n",
    "In this notebook, features will be extracted from the previously annotated dataset. This step allows to train the neural network in the final step.\n",
    "\n",
    "Please note that the current extractors retrieve counts of links between two nodes such as: **source**-**flow**-**sink**. \n",
    "\n",
    "Source and sink nodes are formated according to the AST markup that has been computed in the previous step. For each extractors, a features list are created with `dataset.queue_operation(ExtractorClass, {\"need_map_features\": True})`. This feature list has to be computed only once and serves to determine the list of labels needed for the feature extractor.\n",
    "\n",
    "## 03.a. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_dataset_path = \"../data/dataset_03_feat_extraction\"\n",
    "\n",
    "joern_dataset = Dataset(joern_dataset_path)\n",
    "joern_dataset.queue_operation(CopyDataset, {\"to_path\": fe_dataset_path, \"force\": True})\n",
    "\n",
    "joern_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03.b. Feature extraction\n",
    "\n",
    "This extractor retrieves any node labelled `UpstreamNode` (named **root1**) `n` hops away from any node labelled `DownstreamNode` (named **root2**), and part of the function graph. The number of hops `n` is an integer within the range [`min_hops`, `max_hops`], where `min_hops > 0` and `max_hops > min_hops`. If `max_hops` is -1, the extractor retrieves all possible relationships.\n",
    "\n",
    "Node **root1** is designated as the source, **root2** is the sink and the flow is the relationship between nodes. This chain has the following format: **Ri** which could be `FLOWS_TO`, `REACHES` or `CONTROLS` relationship. Each extracted feature is then counted and added to the feature map. This extractor normalizes the columns (`feat_col = feat_count / entrypoint_count`) for every entrypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_dataset = Dataset(fe_dataset_path)\n",
    "fe_dataset.queue_operation(HopsNFlowsExtractor, {\"min_hops\": 1, \"max_hops\": -1, \"need_map_features\": True})\n",
    "fe_dataset.queue_operation(HopsNFlowsExtractor, {\"min_hops\": 1, \"max_hops\": -1})\n",
    "fe_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.c. Dimension reduction (using PCA)\n",
    "\n",
    "In this step, the number of features previously obtained will be reduced to ensure quick convergence of the model. Several methods exist, either by selecting the most important feature or creating new ones. Here, Principal Component Analysis (PCA) will be used to create a limited number of independant feautures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_dataset.queue_operation(PCA, {\"dimension\": 90, \"dry_run\": True})\n",
    "fe_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Models training\n",
    "\n",
    "In the previous steps, the dataset was curated and several feature extracted to train machine learning models. The models will now be initialized and trained with the curated dataset. The dataset is split 80/20 for training and testing, respectively.\n",
    "\n",
    "### 04.a. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directories\n",
    "lin_dataset_path = \"../data/dataset_04a_lin_cls\"\n",
    "dnn_dataset_path = \"../data/dataset_04b_dnn_cls\"\n",
    "\n",
    "fe_dataset = Dataset(fe_dataset_path)\n",
    "fe_dataset.queue_operation(CopyDataset, {\"to_path\": lin_dataset_path, \"force\": True})\n",
    "fe_dataset.queue_operation(CopyDataset, {\"to_path\": dnn_dataset_path, \"force\": True})\n",
    "\n",
    "fe_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.b. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_dataset = Dataset(lin_dataset_path)\n",
    "lin_dataset.queue_operation(\n",
    "    LinearClassifierTraining, {\"name\": \"lin-cls\", \"max_items\": 1000, \"epochs\": 5, \"reset\": True}\n",
    ")\n",
    "lin_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.c. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_dataset = Dataset(dnn_dataset_path)\n",
    "dnn_dataset.queue_operation(DNNClassifierTraining, {\"name\": \"dnn-default\", \"epochs\": 10, \"reset\": True})\n",
    "dnn_dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Conclusion\n",
    "\n",
    "For more information, please refer to the documentation, available at https://pages.nist.gov/ai-bugfinder-testbed/readme.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
