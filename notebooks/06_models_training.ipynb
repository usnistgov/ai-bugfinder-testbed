{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Models training\n",
    "\n",
    "In the previous notebooks, the dataset were curated and several feature extracted to train various machine learning models. In this notebook, the models will be initialized and trained with the curated datasets. The dataset is split 80/20 for training and testing, respectively.\n",
    "\n",
    "## 06.a. Imports, logging configuration and dataset preparation\n",
    "\n",
    "The first step is to perform the necessary imports and configure the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable these line if live changes in the codebase are made\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tensorflow logging\n",
    "import os\n",
    "import logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific instruction to run the notebooks from a sub-folder.\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from bugfinder.settings import LOGGER\n",
    "from bugfinder.base.dataset import CodeWeaknessClassificationDataset as Dataset\n",
    "from bugfinder.models.dnn_classifier import DNNClassifierTraining\n",
    "from bugfinder.models.linear_classifier import LinearClassifierTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging to only output INFO level messages\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "LOGGER.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directories (DO NOT EDIT)\n",
    "v__2_dataset_path = [\n",
    "    \"../data/ai-dataset_v112\", \"../data/ai-dataset_v122\", \"../data/ai-dataset_v212\", \"../data/ai-dataset_v222\"\n",
    "]\n",
    "v__3_dataset_path = [\n",
    "    \"../data/ai-dataset_v113\", \"../data/ai-dataset_v123\", \"../data/ai-dataset_v213\", \"../data/ai-dataset_v223\"\n",
    "]\n",
    "\n",
    "dataset_to_copy = [\n",
    "    v__2_dataset_path, v__3_dataset_path\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.b. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-04-21 18:10:04][INFO] Processing ../data/ai-dataset_v112...\n",
      "[2022-04-21 18:10:04][INFO] Dataset initialized in 114ms.\n",
      "[2022-04-21 18:10:04][INFO] Operation queue validated in 0ms.\n",
      "[2022-04-21 18:10:04][INFO] Running operation 1/1 (bugfinder.models.linear_classifier.LinearClassifierTraining)...\n",
      "[2022-04-21 18:10:04][INFO] Training LinearClassifierV2 on 270 items over 10 epochs. Testing on 133 items, focusing on f1-score...\n",
      "[2022-04-21 18:10:05][INFO] Training dataset for epoch 1/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnd/envs/py39/lib/python3.9/site-packages/keras/engine/base_layer_v1.py:1684: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-04-21 18:13:53][INFO] Training dataset for epoch 2/10...\n",
      "[2022-04-21 18:17:34][INFO] Training dataset for epoch 3/10...\n",
      "[2022-04-21 18:20:37][INFO] Training dataset for epoch 4/10...\n",
      "[2022-04-21 18:23:36][INFO] Training dataset for epoch 5/10...\n",
      "[2022-04-21 18:26:34][INFO] Training dataset for epoch 6/10...\n",
      "[2022-04-21 18:29:30][INFO] Training dataset for epoch 7/10...\n",
      "[2022-04-21 18:32:24][INFO] Training dataset for epoch 8/10...\n",
      "[2022-04-21 18:35:17][INFO] Training dataset for epoch 9/10...\n",
      "[2022-04-21 18:38:13][INFO] Training dataset for epoch 10/10...\n",
      "[2022-04-21 18:42:04][INFO] Precision: 68.969% (nan%); Recall: 72.932% (nan%); F-score: 69.780% (nan%).\n",
      "[2022-04-21 18:42:04][INFO] 1 operations run in 31m59s.\n"
     ]
    }
   ],
   "source": [
    "for dataset_path in v__2_dataset_path[:1]:\n",
    "    LOGGER.info(\"Processing %s...\" % dataset_path)\n",
    "    dataset = Dataset(dataset_path)\n",
    "    dataset.queue_operation(\n",
    "        LinearClassifierTraining, {\"name\": \"lin-cls\", \"max_items\": 1000, \"epochs\": 10, \"reset\": True}\n",
    "    )\n",
    "    dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.c. Multilayer Perceptron (default size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-04-21 18:42:04][INFO] Processing ../data/ai-dataset_v112...\n",
      "[2022-04-21 18:42:04][INFO] Dataset initialized in 268ms.\n",
      "[2022-04-21 18:42:04][INFO] Operation queue validated in 0ms.\n",
      "[2022-04-21 18:42:04][INFO] Running operation 1/1 (bugfinder.models.dnn_classifier.DNNClassifierTraining)...\n",
      "[2022-04-21 18:42:04][INFO] Training DNNClassifierV2 on 270 items over 10 epochs. Testing on 133 items, focusing on f1-score...\n",
      "[2022-04-21 18:42:04][INFO] Training dataset for epoch 1/10...\n",
      "[2022-04-21 18:42:32][INFO] Training dataset for epoch 2/10...\n",
      "[2022-04-21 18:42:53][INFO] Training dataset for epoch 3/10...\n",
      "[2022-04-21 18:43:15][INFO] Training dataset for epoch 4/10...\n",
      "[2022-04-21 18:43:36][INFO] Training dataset for epoch 5/10...\n",
      "[2022-04-21 18:43:59][INFO] Training dataset for epoch 6/10...\n",
      "[2022-04-21 18:44:24][INFO] Training dataset for epoch 7/10...\n",
      "[2022-04-21 18:44:49][INFO] Training dataset for epoch 8/10...\n",
      "[2022-04-21 18:45:14][INFO] Training dataset for epoch 9/10...\n",
      "[2022-04-21 18:45:38][INFO] Training dataset for epoch 10/10...\n",
      "[2022-04-21 18:46:10][INFO] Precision: 75.721% (nan%); Recall: 75.940% (nan%); F-score: 68.025% (nan%).\n",
      "[2022-04-21 18:46:10][INFO] 1 operations run in 4m06s.\n"
     ]
    }
   ],
   "source": [
    "for dataset_path in v__2_dataset_path[:1]:\n",
    "    LOGGER.info(\"Processing %s...\" % dataset_path)\n",
    "    dataset = Dataset(dataset_path)\n",
    "    dataset.queue_operation(DNNClassifierTraining, {\"name\": \"dnn-default\", \"epochs\": 10, \"reset\": True})\n",
    "    dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.d. Multilayer Perceptron (configured size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-04-21 18:46:10][INFO] Processing ../data/ai-dataset_v113...\n",
      "[2022-04-21 18:46:11][INFO] Dataset initialized in 593ms.\n",
      "[2022-04-21 18:46:11][INFO] Operation queue validated in 0ms.\n",
      "[2022-04-21 18:46:11][INFO] Running operation 1/1 (bugfinder.models.dnn_classifier.DNNClassifierTraining)...\n",
      "[2022-04-21 18:46:11][INFO] Training DNNClassifierV2 on 270 items over 10 epochs. Testing on 133 items, focusing on f1-score...\n",
      "[2022-04-21 18:46:11][INFO] Training dataset for epoch 1/10...\n",
      "[2022-04-21 18:48:34][INFO] Training dataset for epoch 2/10...\n",
      "[2022-04-21 18:50:51][INFO] Training dataset for epoch 3/10...\n",
      "[2022-04-21 18:53:13][INFO] Training dataset for epoch 4/10...\n",
      "[2022-04-21 18:55:34][INFO] Training dataset for epoch 5/10...\n",
      "[2022-04-21 18:57:52][INFO] Training dataset for epoch 6/10...\n",
      "[2022-04-21 19:00:12][INFO] Training dataset for epoch 7/10...\n",
      "[2022-04-21 19:02:42][INFO] Training dataset for epoch 8/10...\n",
      "[2022-04-21 19:04:58][INFO] Training dataset for epoch 9/10...\n",
      "[2022-04-21 19:07:32][INFO] Training dataset for epoch 10/10...\n",
      "[2022-04-21 19:11:03][INFO] Precision: 55.407% (nan%); Recall: 74.436% (nan%); F-score: 63.527% (nan%).\n",
      "[2022-04-21 19:11:03][INFO] 1 operations run in 24m51s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnd/envs/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pnd/envs/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pnd/envs/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for dataset_path in v__3_dataset_path[:1]:\n",
    "    LOGGER.info(\"Processing %s...\" % dataset_path)\n",
    "    dataset = Dataset(dataset_path)\n",
    "    dataset.queue_operation(DNNClassifierTraining, {\"name\": \"dnn-custom\", \"architecture\": [25, 50, 75, 50, 25], \n",
    "                                                    \"epochs\": 10, \"reset\": True})\n",
    "    dataset.process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
