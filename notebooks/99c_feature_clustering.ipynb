{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Feature clustering (experimental)\n",
    "\n",
    "A first attempt at finding the optimal number of cluster for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable these line if live changes in the codebase are made\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific instruction to run the notebooks from a sub-folder.\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from bugfinder.settings import LOGGER\n",
    "from bugfinder.dataset import CWEClassificationDataset as Dataset\n",
    "from bugfinder.dataset.processing.dataset_ops import CopyDataset, RightFixer\n",
    "from bugfinder.features.any_hop.all_flows import FeatureExtractor as AnyHopAllFlowsExtractor\n",
    "from bugfinder.features.any_hop.single_flow import FeatureExtractor as AnyHopSingleFlowExtractor\n",
    "from bugfinder.features.single_hop.raw import FeatureExtractor as SingleHopRawExtractor\n",
    "from bugfinder.features.pca import FeatureExtractor as PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging to only output INFO level messages\n",
    "LOGGER.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directories (DO NOT EDIT)\n",
    "cwe121_v__0_dataset_path = [\n",
    "    \"../data/cwe121_v110\", \"../data/cwe121_v120\", \"../data/cwe121_v210\", \"../data/cwe121_v220\", \n",
    "#     \"../data/cwe121_v310\", \"../data/cwe121_v320\"\n",
    "]\n",
    "cwe121_v__1_dataset_path = [\n",
    "    \"../data/cwe121_v111\", \"../data/cwe121_v121\", \"../data/cwe121_v211\", \"../data/cwe121_v221\", \n",
    "#     \"../data/cwe121_v311\", \"../data/cwe121_v321\"\n",
    "]\n",
    "cwe121_v__2_dataset_path = [\n",
    "    \"../data/cwe121_v112\", \"../data/cwe121_v122\", \"../data/cwe121_v212\", \"../data/cwe121_v222\", \n",
    "#     \"../data/cwe121_v312\", \"../data/cwe121_v322\"\n",
    "]\n",
    "cwe121_v__3_dataset_path = [\n",
    "    \"../data/cwe121_v113\", \"../data/cwe121_v123\", \"../data/cwe121_v213\", \"../data/cwe121_v223\", \n",
    "#     \"../data/cwe121_v313\", \"../data/cwe121_v323\"\n",
    "]\n",
    "# cwe121_v__4_dataset_path = [\n",
    "#     \"../data/cwe121_v114\", \"../data/cwe121_v124\", \"../data/cwe121_v214\", \"../data/cwe121_v224\", \n",
    "#     \"../data/cwe121_v314\", \"../data/cwe121_v324\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, cluster, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(cwe121_v__1_dataset_path[0])\n",
    "feats = dataset.features\n",
    "    \n",
    "labels = feats[\"result\"].to_numpy()\n",
    "del feats[\"name\"]\n",
    "del feats[\"result\"]\n",
    "features = preprocessing.scale(feats.to_numpy())\n",
    "print(\"Feature size: %s\" % str(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_range = range(20, 50)\n",
    "kmeans_args = {\n",
    "    \"init\": \"random\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 1000,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "kmeans_results = list()\n",
    "\n",
    "for k in kmeans_range:\n",
    "    if k % 10 == 0:\n",
    "        print(\"Searching for %d clusters...\" % (k*10))\n",
    "        \n",
    "    kmeans = cluster.KMeans(\n",
    "        n_clusters=k*10,\n",
    "        **kmeans_args\n",
    "    )\n",
    "    \n",
    "    kmeans.fit(features)\n",
    "    kmeans_results.append(kmeans)\n",
    "    \n",
    "print(\"Search done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = [k.inertia_ for k in kmeans_results]\n",
    "sil = [metrics.silhouette_score(features, k.labels_) for k in kmeans_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(kmeans_range, sse)\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(kmeans_range, sil)\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"Sil. score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "from bugfinder.utils.statistics import get_time\n",
    "\n",
    "neo4j_db = Graph(\n",
    "    scheme=\"http\",\n",
    "    host=\"0.0.0.0\",\n",
    "    port=55487,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrypoints_cmd = \"\"\"\n",
    "    MATCH (n)\n",
    "    WHERE n.ast = \"CFGEntryNode\"\n",
    "    RETURN id(n) as id_n\n",
    "\"\"\"\n",
    "start_time = get_time()\n",
    "entrypoints_list = [data[\"id_n\"] for data in neo4j_db.run(entrypoints_cmd).data()]\n",
    "print(\"Found %d entrypoints in %dms\" % (len(entrypoints_list), get_time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_chain_cmd1 = \"\"\"\n",
    "    MATCH (n)-[:FLOWS_TO|CONTROLS|REACHESS*]->(m)\n",
    "    WHERE id(n) = %s\n",
    "    RETURN distinct m as nodes\n",
    "\"\"\"\n",
    "\n",
    "start_time = get_time()\n",
    "\n",
    "exec_chains1 = [neo4j_db.run(exec_chain_cmd1 % entrypoint_id).data() for entrypoint_id in entrypoints_list[:15]]\n",
    "    \n",
    "print(\"Spent %dms\" % (get_time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_chain_cmd2 = \"\"\"\n",
    "    MATCH (n) \n",
    "    WHERE id(n)=%s \n",
    "    CALL apoc.path.subgraphAll(n, {relationshipFilter: \"FLOWS_TO|REACHES|CONTROLS\"})\n",
    "    yield nodes, relationships\n",
    "    return nodes, relationships\n",
    "\"\"\"\n",
    "\n",
    "start_time = get_time()\n",
    "\n",
    "exec_chains2 = [neo4j_db.run(exec_chain_cmd2 % entrypoint_id).data() for entrypoint_id in entrypoints_list]\n",
    "    \n",
    "print(\"Spent %dms\" % (get_time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_nodes = [set([nodes[\"nodes\"] for nodes in chain]) for chain in exec_chains1]\n",
    "ch2_nodes = [set(chain[0][\"nodes\"]) for chain in exec_chains2]\n",
    "ch1_ids = [set([n.identity for n in chain]) for chain in ch1_nodes]\n",
    "ch2_ids = [set([n.identity for n in chain]) for chain in ch2_nodes]\n",
    "\n",
    "all_nodes = ch1_nodes + ch2_nodes\n",
    "node_dict = dict()\n",
    "\n",
    "for node_list in all_nodes:\n",
    "    for node in node_list:\n",
    "        node_dict[str(node.identity)] = node\n",
    "\n",
    "for x in range(len(ch1_ids)):\n",
    "    print(\"Sample: %02d/%02d\" % (x+1, len(ch1_ids)))\n",
    "    ids_diff = ch1_ids[x].union(ch2_ids[x]) - ch1_ids[x].intersection(ch2_ids[x])\n",
    "    \n",
    "    for node_id in ids_diff:\n",
    "        print(\"Id %s not present in both: %s\" % (str(node_id), str(node_dict[str(node_id)])))\n",
    "        \n",
    "    print(\"******************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = exec_chains2[0][0][\"relationships\"][0]\n",
    "\n",
    "print(\"TYPE: %s\" % type(rel).__name__)\n",
    "print(rel.start_node.identity)\n",
    "print(rel.end_node.identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size\n",
    "\n",
    "\n",
    "print(get_size(exec_chains1))\n",
    "print(get_size(exec_chains2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_chains3 = list()\n",
    "\n",
    "start_time = get_time()\n",
    "\n",
    "for chain in exec_chains2:\n",
    "    result_chain = {\n",
    "        \"nodes\": dict(),\n",
    "        \"rels\": list()\n",
    "    }\n",
    "    \n",
    "    for node in chain[0][\"nodes\"]:\n",
    "        result_chain[\"nodes\"][node.identity] = node.get(\"ast\")\n",
    "        \n",
    "    for rel in chain[0][\"relationships\"]:\n",
    "        new_rel = (\n",
    "            rel.start_node.identity,\n",
    "            type(rel).__name__,\n",
    "            rel.end_node.identity\n",
    "        )\n",
    "        result_chain[\"rels\"].append(new_rel)\n",
    "        \n",
    "    exec_chains3.append(result_chain)\n",
    "        \n",
    "print(\"Spent %dms\" % (get_time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the usual ipython objects, including this one you are creating\n",
    "# ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# # Get a sorted list of the objects and their sizes\n",
    "# sorted([\n",
    "#     (x, get_size(globals().get(x))) \n",
    "#     for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], \n",
    "#     key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_size(exec_chains3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
