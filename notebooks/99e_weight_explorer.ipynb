{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Weight Explorer (experimental)\n",
    "\n",
    "A notebook to explore the influence of features on a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tensorflow logging\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific instruction to run the notebooks from a sub-folder.\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from bugfinder.settings import LOGGER\n",
    "\n",
    "from os.path import join, exists, basename, dirname\n",
    "from shutil import rmtree, copytree\n",
    "\n",
    "import tensorflow as tf\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging to only output INFO level messages\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "LOGGER.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_weights(input_weights, log=False):\n",
    "    w_prep = list()\n",
    "    \n",
    "    for value in input_weights.values():\n",
    "        last_col = value.columns[-1]\n",
    "        w_prep.append(\n",
    "            deepcopy(value.drop([last_col], axis=1).values.tolist())\n",
    "        )\n",
    "        \n",
    "    return w_prep\n",
    "\n",
    "def final_weights(input_weights, log=False):\n",
    "    size = [len(layer[0]) for layer in input_weights]\n",
    "    final_weights = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "    iterator = np.nditer(final_weights, flags=['multi_index'])\n",
    "    for _ in iterator:\n",
    "        idx = iterator.multi_index\n",
    "        v = 1\n",
    "        for n in range(len(idx) - 1):\n",
    "            if log:\n",
    "                print(f\"Getting {n},{idx[n+1]},{idx[n]}: {input_weights[n].shape}\")\n",
    "            v *= input_weights[n][idx[n+1]][idx[n]]\n",
    "\n",
    "        final_weights[idx] = v * input_weights[-1][0][idx[-1]]\n",
    "        \n",
    "    return final_weights\n",
    "\n",
    "def revise_node_weights(node_weights, min_bound=0, max_bound=1):\n",
    "    abs_values = np.abs(node_weights)\n",
    "    min_value = np.min(abs_values)\n",
    "    max_value = np.max(abs_values)\n",
    "    \n",
    "    if min_value == max_value:\n",
    "        revised_node_weights = [0 for _ in abs_values]\n",
    "    else:\n",
    "        normalized_weights = [\n",
    "            min_bound + (max_bound - min_bound) * (v - min_value) / (max_value - min_value) \n",
    "            for v in abs_values\n",
    "        ]\n",
    "        threshold = np.median(normalized_weights) - 0.5 * np.std(normalized_weights)\n",
    "\n",
    "        revised_node_weights = [get_val(v, threshold) for v in normalized_weights]\n",
    "    \n",
    "    return revised_node_weights\n",
    "\n",
    "def get_val(val, thr):\n",
    "    return val if val >= thr else 0\n",
    "\n",
    "def init_model(model_dir, model_arch, feat_count):\n",
    "    # Renaming input columns to avoid forbidden characters\n",
    "    columns = [\n",
    "        f\"feat{feature_nb}\" for feature_nb in range(feat_count)\n",
    "    ]\n",
    "\n",
    "    model = tf.estimator.DNNClassifier(\n",
    "        hidden_units = model_arch,\n",
    "        feature_columns=[\n",
    "            tf.feature_column.numeric_column(col) for col in columns\n",
    "        ],\n",
    "        n_classes=2,\n",
    "        model_dir=model_dir,\n",
    "    )\n",
    "\n",
    "    return model, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/pnd/model23\"\n",
    "model_arch = [10, 10, 10]\n",
    "feat_count = 632\n",
    "        \n",
    "model, columns = init_model(\n",
    "    model_dir, \n",
    "    model_arch, \n",
    "    feat_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model_vars = [var for var in model.get_variable_names() if basename(var) == \"kernel\"]\n",
    "layers = sorted(set([basename(dirname(var)) for var in chosen_model_vars]))\n",
    "cols = columns\n",
    "\n",
    "weights = dict()\n",
    "\n",
    "for layer in layers:\n",
    "    kernel = model.get_variable_value(f\"dnn/{layer}/kernel\").transpose()\n",
    "    \n",
    "    layer_weights = np.zeros((kernel.shape[0], kernel.shape[1]+1))\n",
    "    \n",
    "    layer_weights[:, :-1] = kernel\n",
    "    layer_weights[:, -1] = model.get_variable_value(f\"dnn/{layer}/bias\")\n",
    "    \n",
    "    weights[layer] = pd.DataFrame(layer_weights, columns=cols+[\"bias\"])\n",
    "    \n",
    "    cols = [f\"{layer}_n{index}\" for index in range(layer_weights.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: correct layer value\n",
    "new_weights = dict()\n",
    "\n",
    "for layer in layers:\n",
    "    layer_values = weights[layer].values\n",
    "    new_layer_values = list()\n",
    "\n",
    "    for node_values in layer_values:\n",
    "        updated_node_values = revise_node_weights(node_values, -1, 1)\n",
    "        new_layer_values.append(updated_node_values)\n",
    "        \n",
    "    new_weights[layer] = pd.DataFrame(\n",
    "        new_layer_values, columns=[f\"c{idx}\" for idx in range(len(new_layer_values[0]))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights using orig weights\n",
    "pww = prep_weights(weights)\n",
    "fww = final_weights(pww)\n",
    "fww_abs = np.abs(fww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights using normalized weights\n",
    "npw = prep_weights(new_weights)\n",
    "nfw = final_weights(npw)\n",
    "nfw_abs = np.abs(nfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute means\n",
    "fww_avg = list()\n",
    "fww_abs_avg = list()\n",
    "nfw_avg = list()\n",
    "nfw_abs_avg = list()\n",
    "\n",
    "for idx in range(len(fww)):\n",
    "    fww_avg.append(np.mean(fww[idx].flatten()))\n",
    "    fww_abs_avg.append(np.mean(fww_abs[idx].flatten()))\n",
    "    nfw_avg.append(np.mean(nfw[idx].flatten()))\n",
    "    nfw_abs_avg.append(np.mean(nfw_abs[idx].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spread = 100\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "ax[0, 0].hist(fww_avg, spread, color='blue', alpha=0.75)\n",
    "ax[0, 1].hist(fww_abs_avg, spread, color='orange', alpha=0.75)\n",
    "ax[1, 0].hist(nfw_avg, spread, color='blue', alpha=0.75)\n",
    "ax[1, 1].hist(nfw_abs_avg, spread, color='orange', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10]\n",
    "fw_corrs = [deepcopy(fww) for _ in range(len(thresholds))]\n",
    "\n",
    "print(sum([np.count_nonzero(fw_corr.flatten()) for fw_corr in fw_corrs]) - len(fw_corrs[0].flatten())*len(fw_corrs))\n",
    "\n",
    "print(\"Computing...\")\n",
    "for thr_idx, thr_val in tqdm(enumerate(thresholds), total=len(thresholds)):\n",
    "    for fidx, fv in tqdm(enumerate(fww_abs), desc=f\"Threshold: {thr_val}\", total=len(fww_abs), leave=False):\n",
    "        flat = fv.flatten()\n",
    "\n",
    "        threshold = thr_val * np.std(flat)\n",
    "\n",
    "        with np.nditer(fw_corrs[thr_idx][fidx], op_flags=['readwrite']) as it:\n",
    "            for x in it:\n",
    "                if abs(x) < threshold:\n",
    "                    x[...] = 0.\n",
    "\n",
    "print(sum([np.count_nonzero(fw_corr.flatten()) for fw_corr in fw_corrs]) - len(fw_corrs[0].flatten())*len(fw_corrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_corrs_min = []\n",
    "fw_corrs_max = []\n",
    "fig, ax = plt.subplots(nrows=5, ncols=2, figsize=(20, 20))\n",
    "\n",
    "fig_idx = 0\n",
    "ax[int(np.floor(fig_idx/2)), fig_idx%2].hist(fww_avg, spread, alpha=0.75)\n",
    "\n",
    "for fw_corr in fw_corrs:\n",
    "    fw_corr_avg = []\n",
    "    fig_idx += 1\n",
    "    for idx in range(len(fww)):\n",
    "        fw_corr_avg.append(np.mean(fw_corr[idx].flatten()))\n",
    "        \n",
    "    fw_corrs_min.append(np.min(fw_corr_avg))\n",
    "    fw_corrs_max.append(np.max(fw_corr_avg))\n",
    "    \n",
    "    ax[int(np.floor(fig_idx/2)), fig_idx%2].hist(fw_corr_avg, spread, alpha=0.75)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thresholds)\n",
    "print(fw_corrs_min)\n",
    "print(fw_corrs_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_corr = fw_corrs[3]\n",
    "zeros = [(ft.size - np.count_nonzero(ft.flatten())) / ft.size for ft in chosen_corr]\n",
    "\n",
    "plt.hist(zeros, spread, alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cols = np.argsort(zeros)\n",
    "sorted_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_cols = sorted_cols[-150:]\n",
    "limited_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiwe = pd.read_csv(\"../data/aiwe-test-nopca3/sign01.csv\")\n",
    "aiwe_cols = np.array(aiwe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_cols_for_pd = [aiwe_cols[idx] for idx in limited_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_aiwe = aiwe.drop(aiwe.columns.difference(limited_cols_for_pd), axis=1)\n",
    "new_aiwe.to_csv(\"../data/aiwe-test-nopca3/sign01-lim.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
