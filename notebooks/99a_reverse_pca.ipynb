{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse PCA (experimental)\n",
    "\n",
    "A notebook to apply PCA and try reversing it while calculating the overall accuracy of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable these line if live changes in the codebase are made\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tensorflow logging\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific instruction to run the notebooks from a sub-folder.\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from bugfinder.settings import LOGGER\n",
    "from bugfinder.dataset import CWEClassificationDataset as Dataset\n",
    "from bugfinder.models.dnn_classifier import DNNClassifierTraining\n",
    "from bugfinder.models.linear_classifier import LinearClassifierTraining\n",
    "from bugfinder.dataset.processing.dataset_ops import CopyDataset, RightFixer\n",
    "from bugfinder.features.any_hop.all_flows import FeatureExtractor as AnyHopAllFlowsExtractor\n",
    "from bugfinder.features.any_hop.single_flow import FeatureExtractor as AnyHopSingleFlowExtractor\n",
    "from bugfinder.features.single_hop.raw import FeatureExtractor as SingleHopRawExtractor\n",
    "from bugfinder.features.pca import FeatureExtractor as PCA\n",
    "\n",
    "from os.path import join, exists, basename, dirname\n",
    "from shutil import rmtree, copytree\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bugfinder.dataset.processing import DatasetProcessing, DatasetProcessingCategory\n",
    "from bugfinder.settings import LOGGER\n",
    "from bugfinder.utils.statistics import has_better_metrics\n",
    "from os.path import join, exists\n",
    "from shutil import rmtree, copytree\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "from xlsxwriter.utility import xl_rowcol_to_cell\n",
    "import sklearn.decomposition\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging to only output INFO level messages\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "LOGGER.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-16 12:23:41][INFO] Dataset initialized in 46.318s.\n",
      "5004\n"
     ]
    }
   ],
   "source": [
    "# test_dataset = \"../data/cwe121_v112\"\n",
    "test_dataset = \"../data/ds-rev-pca\"\n",
    "dataset = Dataset(test_dataset)\n",
    "\n",
    "orig_cols = list(dataset.features.columns)\n",
    "print(len(orig_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5002\n"
     ]
    }
   ],
   "source": [
    "df = dataset.features\n",
    "df = df.drop(orig_cols[-2:], axis=1)\n",
    "\n",
    "orig_cols = orig_cols[:-2]\n",
    "print(len(orig_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.to_numpy()\n",
    "mu = np.mean(X, axis=0)\n",
    "n_comp = 50\n",
    "\n",
    "# PCA\n",
    "pca = sklearn.decomposition.PCA(n_components=n_comp)\n",
    "pca.fit(X)\n",
    "\n",
    "# Reverse PCA\n",
    "Xhat = np.dot(pca.transform(X)[:,:n_comp], pca.components_[:n_comp,:])\n",
    "Xhat += mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784ae1d8ba9f4b86980479d8d3c6b8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=24005.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing stats...\n",
      "Job done!\n"
     ]
    }
   ],
   "source": [
    "max_items = X.shape[0]\n",
    "# max_items = 1000\n",
    "errors = [[], [], [], []]\n",
    "avg_err_append = errors[0].append\n",
    "med_err_append = errors[1].append\n",
    "std_err_append = errors[2].append\n",
    "est_err_append = errors[3].append\n",
    "\n",
    "for item in tqdm(range(max_items)):    \n",
    "    origX = list(X[item,])\n",
    "    newX = list(Xhat[item,])\n",
    "    assert len(origX)==len(newX)\n",
    "    \n",
    "    base_errors = list()\n",
    "    base_errors_append = base_errors.append\n",
    "    guess_errors = list()\n",
    "    guess_errors_append = guess_errors.append\n",
    "\n",
    "    for idx in range(len(origX)):\n",
    "        orig = float(origX[idx])\n",
    "        new = float(newX[idx])\n",
    "        guess = int(round(new, 0))\n",
    "        \n",
    "        base_errors_append(abs(new-orig))\n",
    "        guess_errors_append(int(abs(guess-orig)!=0))\n",
    "        \n",
    "    \n",
    "    avg_err_append(np.mean(base_errors))\n",
    "    med_err_append(np.median(base_errors))\n",
    "    std_err_append(np.std(base_errors))\n",
    "    est_err_append(sum(guess_errors))\n",
    "        \n",
    "print(\"Computing stats...\")\n",
    "\n",
    "avg_err = np.mean(errors[0])\n",
    "median_err = np.mean(errors[1])\n",
    "std_err = np.mean(errors[2])\n",
    "\n",
    "err_count = sum(errors[3])\n",
    "avg_err_count = np.mean(errors[3])\n",
    "max_err_count = np.max(errors[3])\n",
    "\n",
    "rate = np.array(errors[3]) / X.shape[1]\n",
    "err_rate = np.mean(rate)\n",
    "max_error_rate = np.max(rate)\n",
    "\n",
    "print(\"Job done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reverse PCA on 24005 items with 5002 rows ***\n",
      "\n",
      "===== Error count =====\n",
      "Dataset: 177084 errors / 120073010 (0.15%)\n",
      "\n",
      "Single item: 7.38 errors / item (0.15%)\n",
      "\n",
      "Maximum for a single item: 186 errors(3.72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"*** Reverse PCA on {max_items} items with {X.shape[1]} rows ***\\n\")\n",
    "print(f\"{'=' * 5} Error count {'=' * 5}\")\n",
    "print(\n",
    "    f\"Dataset: {err_count} errors / {max_items * X.shape[1]} \"\n",
    "    f\"({round(100 * err_count / (max_items * X.shape[1]), 2)}%)\\n\"\n",
    ")\n",
    "avg_per_item = round(avg_err_count, 2)\n",
    "print(\n",
    "    f\"Single item: {avg_per_item} errors / item \"\n",
    "    f\"({round(100 * avg_per_item / X.shape[1], 2)}%)\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"Maximum for a single item: {max_err_count} errors\"\n",
    "    f\"({round(100 * max_err_count / X.shape[1], 2)}%)\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 5002)\n",
      "array([[ 2.37334379e-05, -8.47707179e-05,  8.21896327e-06, ...,\n",
      "        -1.17710106e-05,  1.34115350e-05,  3.75208912e-05],\n",
      "       [-4.05347060e-06,  2.68861595e-04,  1.87682802e-05, ...,\n",
      "         3.81902261e-05,  5.04760789e-06, -2.86117900e-04],\n",
      "       [ 5.57580480e-07, -3.87749845e-05, -3.49885766e-06, ...,\n",
      "        -5.26573505e-06, -1.04397156e-06,  2.08760310e-05],\n",
      "       ...,\n",
      "       [ 2.65860682e-04,  1.88672908e-04, -1.14806824e-04, ...,\n",
      "         9.37486740e-05, -2.39431171e-05,  2.84370260e-03],\n",
      "       [ 3.22029657e-04,  4.38529274e-04, -8.87410633e-05, ...,\n",
      "         8.36419110e-05, -2.43278252e-04,  2.67734908e-03],\n",
      "       [ 6.90042863e-05, -5.82700144e-04, -1.55411598e-04, ...,\n",
      "        -1.79962304e-04,  2.04727456e-04,  2.13060166e-03]])\n"
     ]
    }
   ],
   "source": [
    "pca_weights = deepcopy(pca.components_)\n",
    "\n",
    "pprint(pca.components_.shape)\n",
    "pprint(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5002,)\n",
      "array([4.16579879e-05, 7.49843783e-04, 1.66631952e-04, ...,\n",
      "       8.33159758e-05, 8.33159758e-05, 9.99791710e-04])\n"
     ]
    }
   ],
   "source": [
    "pprint(mu.shape)\n",
    "pprint(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 5002)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_weights = np.append([mu], pca_weights, axis=0)\n",
    "pca_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job done!\n"
     ]
    }
   ],
   "source": [
    "workbook  = xlsxwriter.Workbook('/mnt/data/ai_bugfinder/pca.weights.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "cols = df.columns\n",
    "\n",
    "worksheet.write(0, 0, \"Mult.\")\n",
    "worksheet.write(0, 1, 1)\n",
    "\n",
    "useless_fmt = workbook.add_format({'bg_color': '#FFC7CE',\n",
    "                               'font_color': '#9C0006'})\n",
    "useful_fmt = workbook.add_format({'bg_color': '#C6EFCE',\n",
    "                               'font_color': '#006100'})\n",
    "condition_useless = {\n",
    "    'type': 'cell', \n",
    "    'criteria': '<',\n",
    "    'value': None,\n",
    "    'format': None\n",
    "}\n",
    "condition_useful = {\n",
    "    'type': 'cell', \n",
    "    'criteria': '>=',\n",
    "    'value': None,\n",
    "    'format': None\n",
    "}\n",
    "\n",
    "\n",
    "for x in range(pca_weights.shape[0]-1):\n",
    "    worksheet.write(x+3, 0, f\"pca{x}\")\n",
    "    \n",
    "worksheet.write(pca_weights.shape[0]+2, 0, \"mu\")\n",
    "\n",
    "for y in range(len(cols)):\n",
    "    worksheet.write(2, y+1, cols[y])\n",
    "\n",
    "row_idx = 3\n",
    "row_stats = list()\n",
    "\n",
    "for row in pca_weights:\n",
    "    col_idx = 1\n",
    "    row_avg = np.mean(row)\n",
    "    \n",
    "    for item in row:\n",
    "        worksheet.write(row_idx, col_idx, f\"={item}*{xl_rowcol_to_cell(0, 1)}\")\n",
    "        col_idx += 1\n",
    "        \n",
    "    start = xl_rowcol_to_cell(row_idx, 1)\n",
    "    end = xl_rowcol_to_cell(row_idx, col_idx-1)\n",
    "    \n",
    "    row_stats.append(np.argsort(row))\n",
    "    \n",
    "#     current_condition = deepcopy(condition_useless)\n",
    "#     current_condition[\"value\"] = .01 * row_avg\n",
    "#     current_condition[\"format\"] = useless_fmt\n",
    "#     worksheet.conditional_format(f\"{start}:{end}\", current_condition)\n",
    "    \n",
    "#     current_condition = deepcopy(condition_useful)\n",
    "#     current_condition[\"value\"] = .99 * row_avg\n",
    "#     current_condition[\"format\"] = useful_fmt\n",
    "#     worksheet.conditional_format(f\"{start}:{end}\", current_condition)\n",
    "#     worksheet.conditional_format(f\"{start}:{end}\", {\n",
    "#         \"type\": \"3_color_scale\",\n",
    "#         \"min_color\": \"red\",\n",
    "#         \"mid_color\": \"white\",\n",
    "#         \"max_color\": \"green\"\n",
    "#     })\n",
    "    worksheet.conditional_format(f\"{start}:{end}\", {\n",
    "        \"type\": \"2_color_scale\",\n",
    "        \"min_color\": \"white\",\n",
    "        \"max_color\": \"green\"\n",
    "    })\n",
    "    row_idx += 1\n",
    "\n",
    "workbook.close()\n",
    "print(\"Job done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {col: [] for col in cols}\n",
    "percentage = len(cols) / 100\n",
    "\n",
    "for col_idx in range(len(cols)):\n",
    "    for stat in row_stats:\n",
    "#         summary[cols[col_idx]] = int(round(np.where(stat == col_idx)[0] / percentage, 0))\n",
    "        summary[cols[col_idx]].append(int(np.where(stat == col_idx)[0][0] / percentage))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.434066490886106\n"
     ]
    }
   ],
   "source": [
    "# print(np.mean(list(summary.values())))\n",
    "stds = list()\n",
    "\n",
    "for values in summary.values():\n",
    "    stds.append(np.std(values))\n",
    "\n",
    "print(np.mean(stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.to_numpy()\n",
    "mu = np.mean(X, axis=0)\n",
    "\n",
    "n_comp = 50\n",
    "pca = sklearn.decomposition.PCA(n_components=n_comp)\n",
    "pca.fit(X)\n",
    "\n",
    "Xhat = np.dot(pca.transform(X)[:,:n_comp], pca.components_[:n_comp,:])\n",
    "Xhat += mu\n",
    "\n",
    "workbook = xlsxwriter.Workbook(\"/mnt/data/ai_bugfinder/pca.xlsx\")\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "# Error correction test number\n",
    "worksheet.write(1, 0, \"Correction\")\n",
    "worksheet.write(1, 1, 0)\n",
    "\n",
    "offset = 5\n",
    "mean_xy = (offset, 0)\n",
    "mean_xy_title = mean_xy[0]-2\n",
    "mean_xy_avg = mean_xy[0]-1\n",
    "\n",
    "# worksheet.write(mean_xy_title, mean_xy[1]+1, \"avg err\")\n",
    "# worksheet.write(mean_xy_title, mean_xy[1]+2, \"err count\")\n",
    "# worksheet.write(mean_xy_title, mean_xy[1]+3, \"err rate\")\n",
    "# worksheet.write(mean_xy_title, mean_xy[1]+4, \"avg err\")\n",
    "# worksheet.write(mean_xy_title, mean_xy[1]+5, \"err count\")\n",
    "# worksheet.write(mean_xy_title, mean_xy[1]+6, \"err rate\")\n",
    "\n",
    "# worksheet.write(mean_xy_avg, mean_xy[1], \"avg.\")\n",
    "# worksheet.write(\n",
    "#     mean_xy_avg, mean_xy[1]+1, \n",
    "#     f\"=AVERAGE({xl_rowcol_to_cell(mean_xy[0], mean_xy[1]+1)}:{xl_rowcol_to_cell(mean_xy[0]+5000, mean_xy[1]+1)})\"\n",
    "# )\n",
    "# worksheet.write(\n",
    "#     mean_xy_avg, mean_xy[1]+2, \n",
    "#     f\"=AVERAGE({xl_rowcol_to_cell(mean_xy[0], mean_xy[1]+2)}:{xl_rowcol_to_cell(mean_xy[0]+5000, mean_xy[1]+2)})\"\n",
    "# )\n",
    "# worksheet.write(\n",
    "#     mean_xy_avg, mean_xy[1]+3, \n",
    "#     f\"=AVERAGE({xl_rowcol_to_cell(mean_xy[0], mean_xy[1]+3)}:{xl_rowcol_to_cell(mean_xy[0]+5000, mean_xy[1]+3)})\"\n",
    "# )\n",
    "# worksheet.write(\n",
    "#     mean_xy_avg, mean_xy[1]+4, \n",
    "#     f\"=AVERAGE({xl_rowcol_to_cell(mean_xy[0], mean_xy[1]+4)}:{xl_rowcol_to_cell(mean_xy[0]+5000, mean_xy[1]+4)})\"\n",
    "# )\n",
    "# worksheet.write(\n",
    "#     mean_xy_avg, mean_xy[1]+5, \n",
    "#     f\"=AVERAGE({xl_rowcol_to_cell(mean_xy[0], mean_xy[1]+5)}:{xl_rowcol_to_cell(mean_xy[0]+5000, mean_xy[1]+5)})\"\n",
    "# )\n",
    "# worksheet.write(\n",
    "#     mean_xy_avg, mean_xy[1]+6, \n",
    "#     f\"=AVERAGE({xl_rowcol_to_cell(mean_xy[0], mean_xy[1]+6)}:{xl_rowcol_to_cell(mean_xy[0]+5000, mean_xy[1]+6)})\"\n",
    "# )\n",
    "\n",
    "max_items = X.shape[0]\n",
    "\n",
    "for item in tqdm(range(max_items)):    \n",
    "    origX = list(X[item,])\n",
    "    newX = list(Xhat[item,])\n",
    "\n",
    "    assert len(origX)==len(newX)\n",
    "    \n",
    "#     col = int(2*offset*item + offset + 4)\n",
    "\n",
    "    for idx in range(len(origX)):\n",
    "        orig = float(origX[idx])\n",
    "        new = float(newX[idx])\n",
    "        \n",
    "        guess = int(round(new, 0))\n",
    "        \n",
    "        \n",
    "#         row = idx + offset\n",
    "        \n",
    "#         worksheet.write(row, col, orig)\n",
    "#         worksheet.write(row, col+1, new)\n",
    "#         worksheet.write(row, col+2, f\"=ABS({xl_rowcol_to_cell(row, col)}-{xl_rowcol_to_cell(row, col+1)})\")\n",
    "#         worksheet.write(row, col+3, f\"=ROUND({xl_rowcol_to_cell(row, col+1)}, 0)\")\n",
    "#         worksheet.write(row, col+4, f\"={xl_rowcol_to_cell(row, col)}<>{xl_rowcol_to_cell(row, col+3)}\")\n",
    "#         worksheet.write(\n",
    "#             row, col+5, \n",
    "#             f\"=SIGN({xl_rowcol_to_cell(row, col+1)})*(\"\n",
    "#             f\"ABS({xl_rowcol_to_cell(row, col+1)})+{xl_rowcol_to_cell(1, 1)})\"\n",
    "#         )\n",
    "#         worksheet.write(row, col+6, f\"=ABS({xl_rowcol_to_cell(row, col)}-{xl_rowcol_to_cell(row, col+5)})\")\n",
    "#         worksheet.write(row, col+7, f\"=ROUND({xl_rowcol_to_cell(row, col+5)}, 0)\")\n",
    "#         worksheet.write(row, col+8, f\"={xl_rowcol_to_cell(row, col)}<>{xl_rowcol_to_cell(row, col+7)}\")\n",
    "    \n",
    "#     worksheet.write(\n",
    "#         mean_xy[0]+item, mean_xy[1], \n",
    "#         f\"item{item}\"\n",
    "#     )\n",
    "#     worksheet.write(\n",
    "#         mean_xy[0]+item, mean_xy[1]+1, \n",
    "#         f\"=AVERAGE({xl_rowcol_to_cell(offset, col+2)}:{xl_rowcol_to_cell(offset+len(origX), col+2)})\"\n",
    "#     )\n",
    "#     worksheet.write(\n",
    "#         mean_xy[0]+item, mean_xy[1]+2, \n",
    "#         f\"=COUNTIF({xl_rowcol_to_cell(offset, col+4)}:{xl_rowcol_to_cell(offset+len(origX), col+4)}, TRUE)\"\n",
    "#     )\n",
    "#     worksheet.write(\n",
    "#         mean_xy[0]+item, mean_xy[1]+3, \n",
    "#         f\"={xl_rowcol_to_cell(mean_xy[0]+item, mean_xy[1]+2)}/{len(origX)}\"\n",
    "#     )\n",
    "#     worksheet.write(\n",
    "#         mean_xy[0]+item, mean_xy[1]+4, \n",
    "#         f\"=AVERAGE({xl_rowcol_to_cell(offset, col+6)}:{xl_rowcol_to_cell(offset+len(origX), col+6)})\"\n",
    "#     )\n",
    "#     worksheet.write(\n",
    "#         mean_xy[0]+item, mean_xy[1]+5, \n",
    "#         f\"=COUNTIF({xl_rowcol_to_cell(offset, col+8)}:{xl_rowcol_to_cell(offset+len(origX), col+8)}, TRUE)\"\n",
    "#     )\n",
    "#     worksheet.write(\n",
    "#         mean_xy[0]+item, mean_xy[1]+6, \n",
    "#         f\"={xl_rowcol_to_cell(mean_xy[0]+item, mean_xy[1]+5)}/{len(origX)}\"\n",
    "#     )\n",
    "    \n",
    "workbook.close()\n",
    "print(\"Job done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
