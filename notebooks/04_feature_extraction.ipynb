{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Feature extraction \n",
    "\n",
    "In this notebook, features will be extracted from the [previously annotated](./03_neo4j_processing.ipynb). This step allows to train the neural network in the final step.\n",
    "\n",
    "Please note that the current extractors retrieve counts of links between two nodes such as: **source**-**flow**-**sink**. \n",
    "\n",
    "Source and sink nodes are formated according to the AST markup that has been computed in step in the [previous notebook](./03_neo4j_processing.ipynb). For each extractors, a features list are created with `dataset.queue_operation(ExtractorClass, {\"need_map_features\": True})`. This feature list has to be computed only once and serves to determine the list of labels needed for the feature extractor.\n",
    "\n",
    "## 04.a. Imports, logging configuration and dataset preparation\n",
    "\n",
    "The first step is to perform the necessary imports and configure the program. Additionnally, the previously used datasets are copied into 3 different datasets to have their features extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable these line if live changes in the codebase are made\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific instruction to run the notebooks from a sub-folder.\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from bugfinder.settings import LOGGER\n",
    "from bugfinder.dataset import CWEClassificationDataset as Dataset\n",
    "from bugfinder.dataset.processing.dataset_ops import CopyDataset, RightFixer\n",
    "from bugfinder.features.extraction.any_hop.all_flows import FeatureExtractor as AnyHopAllFlowsExtractor\n",
    "from bugfinder.features.extraction.any_hop.single_flow import FeatureExtractor as AnyHopSingleFlowExtractor\n",
    "from bugfinder.features.extraction.single_hop.raw import FeatureExtractor as SingleHopRawExtractor\n",
    "from bugfinder.features.extraction.hops_n_flows import FeatureExtractor as HopsNFlowsExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging to only output INFO level messages\n",
    "LOGGER.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directories (DO NOT EDIT)\n",
    "v__0_dataset_path = [\n",
    "    \"../data/ai-dataset_v110\", \"../data/ai-dataset_v120\", \"../data/ai-dataset_v210\", \"../data/ai-dataset_v220\", \n",
    "]\n",
    "# v__1_dataset_path = [\n",
    "#     \"../data/ai-dataset_v111\", \"../data/ai-dataset_v121\", \"../data/ai-dataset_v211\", \"../data/ai-dataset_v221\",\n",
    "# ]\n",
    "v__2_dataset_path = [\n",
    "    \"../data/ai-dataset_v112\", \"../data/ai-dataset_v122\", \"../data/ai-dataset_v212\", \"../data/ai-dataset_v222\",\n",
    "]\n",
    "v__3_dataset_path = [\n",
    "    \"../data/ai-dataset_v113\", \"../data/ai-dataset_v123\", \"../data/ai-dataset_v213\", \"../data/ai-dataset_v223\",\n",
    "]\n",
    "\n",
    "dataset_to_copy = [\n",
    "#     v__1_dataset_path, v__2_dataset_path, v__3_dataset_path,\n",
    "    v__2_dataset_path, v__3_dataset_path\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the necessary dataset clones\n",
    "from os.path import basename\n",
    "\n",
    "LOGGER.info(\"Starting datasets copy...\")\n",
    "LOGGER.setLevel(logging.WARNING)\n",
    "\n",
    "for index in range(len(v__0_dataset_path)):\n",
    "    dataset = Dataset(v__0_dataset_path[index])\n",
    "    \n",
    "    for dataset_paths in dataset_to_copy:\n",
    "        dataset.queue_operation(CopyDataset, {\"to_path\": dataset_paths[index], \"force\": True})\n",
    "        \n",
    "    dataset.process()\n",
    "    print(\"Dataset %s copied.\" % basename(v__0_dataset_path[index]))\n",
    "    \n",
    "LOGGER.setLevel(logging.INFO)\n",
    "LOGGER.info(\"Datasets copied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.b. AnyHop AllFlow (DEPRECATED)\n",
    "\n",
    "This extractor retrieves any node labelled `UpstreamNode` (named **root1**) up to 5 hops away from the function entrypoint. Then, any node labelled `DownstreamNode` (named **root2**), located up to 3 hops away from **root1** is extracted. Node **root1** is designated as the source, **root2** is the sink and the flow is a chain of the relationships between nodes. This chain has the following format: **R1:R2:...:Rn**, where **Ri** could be `FLOWS_TO`, `REACHES` or `CONTROLS` relationship. Each extracted feature is then added to the feature map as single item.\n",
    "\n",
    "**Warning:** The AnyHopAllFlowsExtractor is being deprecated and will be removed in a future version. This part of the notebook will be removed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_path in v__1_dataset_path:\n",
    "#     LOGGER.info(\"Processing %s...\" % dataset_path)\n",
    "#     dataset = Dataset(dataset_path)\n",
    "#     dataset.queue_operation(AnyHopAllFlowsExtractor, {\"need_map_features\": True})\n",
    "#     dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_path in v__1_dataset_path:\n",
    "#     LOGGER.info(\"Processing %s...\" % dataset_path)\n",
    "#     dataset = Dataset(dataset_path)\n",
    "#     dataset.queue_operation(AnyHopAllFlowsExtractor)\n",
    "#     dataset.queue_operation(PCA, {\"final_dimension\": 50})\n",
    "#     dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.c. SingleHop\n",
    "\n",
    "This extractor retrieves any node labelled `UpstreamNode` (named **root1**) 1 hop away from any node labelled `DownstreamNode` (named **root2**), and part of the function graph. Node **root1** is designated as the source, **root2** is the sink and the flow is the relationship between nodes. This chain has the following format: **Ri** which could be `FLOWS_TO`, `REACHES` or `CONTROLS` relationship. Each extracted feature is then counted and added to the feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_path in v__2_dataset_path:\n",
    "    LOGGER.info(\"Processing %s...\" % dataset_path)\n",
    "    dataset = Dataset(dataset_path)\n",
    "    dataset.queue_operation(SingleHopRawExtractor, {\"need_map_features\": True})\n",
    "    dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset_path in v__2_dataset_path:\n",
    "    LOGGER.info(\"Processing %s...\" % dataset_path)\n",
    "    dataset = Dataset(dataset_path)\n",
    "    dataset.queue_operation(SingleHopRawExtractor)\n",
    "#     dataset.queue_operation(PCA, {\"final_dimension\": 50})\n",
    "    dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.d. AnyHop SingleFlow\n",
    "\n",
    "This extractor retrieves any node labelled `UpstreamNode` (named **root1**) n hop away from any node labelled `DownstreamNode` (named **root2**), and part of the function graph. Node **root1** is designated as the source, **root2** is the sink and the flow is the relationship between nodes. This chain has the following format: **Ri** which could be `FLOWS_TO`, `REACHES` or `CONTROLS` relationship. Each extracted feature is then counted and added to the feature map. This extractor normalizes the columns (`feat_col = feat_count / test_case_count`) for every entrypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_path in v__3_dataset_path:\n",
    "    LOGGER.info(\"Processing %s...\" % dataset_path)\n",
    "    dataset = Dataset(dataset_path)\n",
    "    dataset.queue_operation(AnyHopSingleFlowExtractor, {\"need_map_features\": True})\n",
    "    dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_path in v__3_dataset_path:\n",
    "    LOGGER.info(\"Processing %s...\" % dataset_path)\n",
    "    dataset = Dataset(dataset_path)\n",
    "    dataset.queue_operation(AnyHopSingleFlowExtractor)\n",
    "#     dataset.queue_operation(PCA, {\"final_dimension\": 50})\n",
    "    dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.e. Hops and flows\n",
    "\n",
    "This extractor retrieves any node labelled `UpstreamNode` (named **root1**) `n` hops away from any node labelled `DownstreamNode` (named **root2**), and part of the function graph. The number of hops `n` is an integer within the range [`min_hops`, `max_hops`], where `min_hops > 0` and `max_hops > min_hops`. If `max_hops` is -1, the extractor retrieves all possible relationship.\n",
    "\n",
    "Node **root1** is designated as the source, **root2** is the sink and the flow is the relationship between nodes. This chain has the following format: **Ri** which could be `FLOWS_TO`, `REACHES` or `CONTROLS` relationship. Each extracted feature is then counted and added to the feature map. This extractor normalizes the columns (`feat_col = feat_count / test_case_count`) for every entrypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_path in v__3_dataset_path:\n",
    "    LOGGER.info(\"Processing %s...\" % dataset_path)\n",
    "    dataset = Dataset(dataset_path)\n",
    "    dataset.queue_operation(HopsNFlowsExtractor, {\"min_hops\": 1, \"max_hops\": -1, \"need_map_features\": True})\n",
    "    dataset.queue_operation(HopsNFlowsExtractor, {\"max_hops\": -1})\n",
    "    dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_path in v__3_dataset_path:\n",
    "    LOGGER.info(\"Processing %s...\" % dataset_path)\n",
    "    dataset = Dataset(dataset_path)\n",
    "    dataset.queue_operation(HopsNFlowsExtractor, {\"min_hops\": 1, \"max_hops\": -1})\n",
    "    dataset.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, the previously annotated datasets had several feature extracted. The [next notebook](./05_dimension_reduction.ipynb) trains the models and retrieve the results obtained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
